# Module configuration - model-specific parameters
module_config:
  module: "dit"
  model_name: "ditxl2"
  image_size: 256
  pretrained: false
  learning_rate: 0.0004
  checkpoint: null

# Data module configuration - data-specific parameters
datamodule_config:
  dataset: "imagenet"
  batch_size: 128  # Per-GPU batch size (256 * 2 GPUs = 512 global batch size)
  num_workers: 8
  image_size: 256

# Training configuration - trainer-specific parameters
train_config:
  max_epochs: 80
  accumulate_grad_batches: 1  # No gradient accumulation needed with multi-GPU
  precision: "bf16-mixed"  # Fixed precision setting
  accelerator: "gpu"
  devices: 2  # ✅ Changed from "0,1" to 2 for torchrun compatibility
  strategy: 'ddp_find_unused_parameters_true'  # Will be handled in code to use DDPStrategy with find_unused_parameters=True
  enable_progress_bar: false

# Model checkpoint configuration - checkpoint saving parameters
model_checkpoint_config:
  save_dir: "checkpoints/dit/imagenet"  # ✅ Fixed path (was cifar100, should be imagenet)
  monitor: "val_loss"
  filename: "{epoch:02d}-{val_loss:.2f}"
  save_top_k: 3
  mode: "min"
  save_last: true

# Logger configuration - logging parameters
logger_config:
  type: "wandb"
  project: "dit"
  save_dir: "/scratch/oeg1n18"
  log_model: false

# Quantization configuration - quantization-specific parameters
quantization_config:
  quant_type: null